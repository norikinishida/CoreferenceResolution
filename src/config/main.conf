path {
    storage = /home/nishida/storage/projects/discourse/CoreferenceResolution
    data = ${path.storage}/data
    results = ${path.storage}/results
    caches = ${path.storage}/caches

    craft = /home/nishida/storage/dataset/CRAFT.v4
}

base_hyperparams = ${path}{
    # Model-related configuration
    dropout_rate = 0.3
    ffnn_depth = 1
    ffnn_dim = 3000
    feature_dim = 20
    max_num_speakers = 20
    fine_grained = true
    use_head_attn = true

    # Learning-related configuration
    mention_loss_coef = 0
    batch_size = 1
    max_epoch = 24
    adam_eps = 1e-6
    adam_weight_decay = 1e-2
    warmup_ratio = 0.1
    max_grad_norm = 1.0 # Set 0 to disable clipping
    valid_frequency = 1000

    # Search-space-related configuration
    max_span_width = 20
    max_num_extracted_spans = 3900
    top_span_ratio = 0.4
    max_top_antecedents = 50

    # Data-related configuration
    max_training_sentences = 1000

    # Others
    genres = ["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
}

###########################
# OntoNotes
###########################

joshi2020_spanbert_base_ontonotes = ${base_hyperparams}{
    # Model-related configuration
    model_name = joshi2020
    bert_tokenizer_name = bert-base-cased
    bert_pretrained_name_or_path = ${path.storage}/spanbert_hf_base

    # Learning-related configuration
    bert_learning_rate = 2e-05
    task_learning_rate = 0.0001

    # Data-related configuration
    dataset = ontonotes
    max_segment_len = 384
    truncation_size = 3
}

joshi2020_spanbert_large_ontonotes = ${base_hyperparams}{
    # Model-related configuration
    model_name = joshi2020
    bert_tokenizer_name = bert-base-cased
    bert_pretrained_name_or_path = ${path.storage}/spanbert_hf

    # Learning-related configuration
    bert_learning_rate = 1e-05
    task_learning_rate = 0.0003

    # Data-related configuration
    dataset = ontonotes
    max_segment_len = 512
    truncation_size = 3
}

###########################
# CRAFT
###########################

joshi2020_spanbert_large_craft = ${base_hyperparams}{
    # Model-related configuration
    model_name = joshi2020
    bert_tokenizer_name = microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext
    bert_pretrained_name_or_path = microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext
    use_head_attn = false

    # Learning-related configuration
    bert_learning_rate = 1e-05
    task_learning_rate = 0.0003
    max_epoch = 1000

    # Data-related configuration
    dataset = craft
    max_segment_len = 512
    truncation_size = 3
}

###########################
# Debug
###########################

debug = ${base_hyperparams}{
    # Model-related configuration
    model_name = joshi2020
    bert_tokenizer_name = bert-base-cased
    bert_pretrained_name_or_path = ${path.storage}/spanbert_hf_base

    # Learning-related configuration
    bert_learning_rate = 2e-05
    task_learning_rate = 0.0001

    # Data-related configuration
    dataset = ontonotes
    max_segment_len = 384
    truncation_size = 3
}


